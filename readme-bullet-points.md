- TODO
  - Read through generated readme, check, add, smooth out
  - Fix formula
  - Rewrite script with all configs, check name vs filename
  - Fix names in reproduction instructions
- Goal
  - Reproduce scaling law optimal loss vs non embedding compute with unlimited data
  - Use a small amount of compute by only using small models
  - Learn to use methods, get raw results
- Means
  - Use small models for limited compute budget
  - Colab notebook
  - A100 GPU for roughly 30h
  - Dataset allenai/c4
  - Using hugging face deep learning framework
- Results
  - In the log log plot the curves have a kink, from fast learning to slow learning
  - The linear relationship of the convex hull is clearly visible
  - The linear fit gives a power law L(C) = (C/(6.6*10^4))^-0.0717, cp with L(C) = (C/(2.3*10^8))^-0.050
  - The smaller models are less compute efficient
  - The small models learn more slowly from the beginning
  - Differences: Same batch size for all runs (32), only latent dimension was changes, not number of layers
  - Speed of learning: 50k-150k tokens / s
  - Optimizations: flash attention 2, bf16
